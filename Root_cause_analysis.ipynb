{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ee88d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                   int64\n",
      "CPU_LOAD             int64\n",
      "MEMORY_LEAK_LOAD     int64\n",
      "DELAY                int64\n",
      "ERROR_1000           int64\n",
      "ERROR_1001           int64\n",
      "ERROR_1002           int64\n",
      "ERROR_1003           int64\n",
      "ROOT_CAUSE          object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CPU_LOAD</th>\n",
       "      <th>MEMORY_LEAK_LOAD</th>\n",
       "      <th>DELAY</th>\n",
       "      <th>ERROR_1000</th>\n",
       "      <th>ERROR_1001</th>\n",
       "      <th>ERROR_1002</th>\n",
       "      <th>ERROR_1003</th>\n",
       "      <th>ROOT_CAUSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MEMORY_LEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MEMORY_LEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MEMORY_LEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MEMORY_LEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NETWORK_DELAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  CPU_LOAD  MEMORY_LEAK_LOAD  DELAY  ERROR_1000  ERROR_1001  ERROR_1002  \\\n",
       "0   1         0                 0      0           0           1           0   \n",
       "1   2         0                 0      0           0           0           0   \n",
       "2   3         0                 1      1           0           0           1   \n",
       "3   4         0                 1      0           1           1           0   \n",
       "4   5         1                 1      0           1           0           1   \n",
       "\n",
       "   ERROR_1003     ROOT_CAUSE  \n",
       "0           1    MEMORY_LEAK  \n",
       "1           1    MEMORY_LEAK  \n",
       "2           1    MEMORY_LEAK  \n",
       "3           1    MEMORY_LEAK  \n",
       "4           0  NETWORK_DELAY  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Load Data and review content\n",
    "df = pd.read_csv(\"root_cause_analysis.csv\")\n",
    "#print(\"\\nLoaded Data :\\n------------------------------------\")\n",
    "print(df.dtypes)\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78685955",
   "metadata": {},
   "source": [
    "Let's convert the inputdata so that the machine learning algorithm can read easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1707d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1.    0.    0. ...    0.    1.    1.]\n",
      " [   2.    0.    0. ...    0.    1.    1.]\n",
      " [   3.    0.    1. ...    1.    1.    1.]\n",
      " ...\n",
      " [ 998.    1.    1. ...    0.    0.    1.]\n",
      " [ 999.    0.    1. ...    0.    0.    2.]\n",
      " [1000.    1.    0. ...    1.    0.    0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import train_test_split\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df['ROOT_CAUSE'] = label_encoder.fit_transform(\n",
    "                                df['ROOT_CAUSE'])\n",
    "\n",
    "#Convert Pandas DataFrame to a numpy vector\n",
    "np_df = df.to_numpy().astype(float)\n",
    "print(np_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ecb34da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature variables : (900, 7)\n",
      "Shape of target variable : (900, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Extract the feature variables (X)\n",
    "X_data = np_df[:,1:8] # we have 8 features \n",
    "#Extract the target variable (Y), conver to one-hot-encodign\n",
    "Y_data=np_df[:,8] #making a separate column for each category and using 1s and 0s \n",
    "                  #to show which category each item belongs to\n",
    "Y_data = tf.keras.utils.to_categorical(Y_data,3)    \n",
    "\n",
    "#Split training and test data\n",
    "X_train,X_test,Y_train,Y_test = train_test_split( X_data, Y_data, test_size=0.10)\n",
    "\n",
    "print(\"Shape of feature variables :\", X_train.shape)\n",
    "print(\"Shape of target variable :\",Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90ff4d",
   "metadata": {},
   "source": [
    "Now, Lets Build and Evaluate the data \n",
    "Building the Model: This involves designing the model's architecture (like layers and neurons) and configuring it (like setting the loss function and optimizer). This step is crucial to tailor the model to the\n",
    "specific characteristics and requirements of the data and task at hand.\n",
    "Evaluate the Model: After training you evaluate the data with the different data set to acess it performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3384d266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense-Layer-1 (Dense)       (None, 128)               1024      \n",
      "                                                                 \n",
      " Dense-Layer-2 (Dense)       (None, 128)               16512     \n",
      "                                                                 \n",
      " Final (Dense)               (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17923 (70.01 KB)\n",
      "Trainable params: 17923 (70.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.9046 - accuracy: 0.7028 - val_loss: 0.6709 - val_accuracy: 0.8444\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6402 - accuracy: 0.8069 - val_loss: 0.4995 - val_accuracy: 0.8278\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.8097 - val_loss: 0.4615 - val_accuracy: 0.8444\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4976 - accuracy: 0.8222 - val_loss: 0.4263 - val_accuracy: 0.8556\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4774 - accuracy: 0.8278 - val_loss: 0.3868 - val_accuracy: 0.8444\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.8319 - val_loss: 0.3855 - val_accuracy: 0.8444\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4555 - accuracy: 0.8361 - val_loss: 0.3766 - val_accuracy: 0.8611\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.8333 - val_loss: 0.3705 - val_accuracy: 0.8500\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.8361 - val_loss: 0.3987 - val_accuracy: 0.8611\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4408 - accuracy: 0.8417 - val_loss: 0.3691 - val_accuracy: 0.8556\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8472 - val_loss: 0.3712 - val_accuracy: 0.8556\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.8361 - val_loss: 0.3544 - val_accuracy: 0.8556\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4260 - accuracy: 0.8361 - val_loss: 0.3560 - val_accuracy: 0.8667\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4245 - accuracy: 0.8458 - val_loss: 0.3723 - val_accuracy: 0.8611\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8486 - val_loss: 0.3573 - val_accuracy: 0.8556\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4214 - accuracy: 0.8417 - val_loss: 0.3762 - val_accuracy: 0.8611\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4112 - accuracy: 0.8458 - val_loss: 0.3583 - val_accuracy: 0.8667\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.8403 - val_loss: 0.3606 - val_accuracy: 0.8611\n",
      "Epoch 19/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4037 - accuracy: 0.8528 - val_loss: 0.3559 - val_accuracy: 0.8722\n",
      "Epoch 20/20\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8375 - val_loss: 0.3562 - val_accuracy: 0.8833\n",
      "\n",
      "Evaluation against Test Dataset :\n",
      "------------------------------------\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3842 - accuracy: 0.8400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3841954469680786, 0.8399999737739563]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "# Keras API from TensorFlow, a high-level neural networks library, for building and training deep learning models\n",
    "from tensorflow.keras import optimizers\n",
    "#optimization algorithms like SGD, Adam, etc., which are methods used to change the attributes of the neural \n",
    "#network such as weights and learning rate in order to reduce losses.\n",
    "from tensorflow.keras.regularizers import l2\n",
    "#which helps prevent overfitting by penalizing large weights in the model's learning process\n",
    "\n",
    "\n",
    "#Setup Training Parameters\n",
    "EPOCHS=20 ## The entire dataset will pass through the network in 20 times\n",
    "\n",
    "BATCH_SIZE=64 #Determines how many data points are processed before the model's internal parameters are updated\n",
    "\n",
    "VERBOSE=1 #Controls the level of detail shown during the training process.\n",
    "#A setting of 1 displays a detailed progress bar and metrics for each epoch.\n",
    "\n",
    "#ensuring the model's output layer has the correct number of neurons to match the number of distinct categories in the data\n",
    "OUTPUT_CLASSES=len(label_encoder.classes_) \n",
    "N_HIDDEN=128\n",
    "VALIDATION_SPLIT=0.2\n",
    "#Create a Keras sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "#Add a Dense Layer\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                             input_shape=(7,),\n",
    "                              name='Dense-Layer-1',\n",
    "                              activation='relu'))\n",
    "\n",
    "#Add a second dense layer\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                              name='Dense-Layer-2',\n",
    "                              activation='relu'))\n",
    "\n",
    "#Add a softmax layer for categorial prediction\n",
    "model.add(keras.layers.Dense(OUTPUT_CLASSES,\n",
    "                             name='Final',\n",
    "                             activation='softmax'))\n",
    "\n",
    "#Compile the model\n",
    "model.compile(\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Build the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "\n",
    "#Evaluate the model against the test dataset and print results\n",
    "print(\"\\nEvaluation against Test Dataset :\\n------------------------------------\")\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125c576",
   "metadata": {},
   "source": [
    "Predicting the root cause "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5acfa1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#Pass individual flags to Predict the root cause\n",
    "import numpy as np\n",
    "\n",
    "CPU_LOAD=1 # Variable Initilization \n",
    "MEMORY_LOAD=0 #Model prediction \n",
    "DELAY=0\n",
    "ERROR_1000=0\n",
    "ERROR_1001=1\n",
    "ERROR_1002=1\n",
    "ERROR_1003=0\n",
    "\n",
    "prediction=np.argmax(model.predict(\n",
    "    [[CPU_LOAD,MEMORY_LOAD,DELAY,\n",
    "      ERROR_1000,ERROR_1001,ERROR_1002,ERROR_1003]]), axis=1 )\n",
    "\n",
    "print(label_encoder.inverse_transform(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d38c653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(label_encoder.inverse_transform(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "In machine learning models, especially in classification tasks, binary (0 and 1) encoding like this is commonly \n",
    "used to represent the presence or absence of a condition, the state of a system, categorical variables, and so on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e2b91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "[0 2 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Predicting as a Batch\n",
    "print(label_encoder.inverse_transform(np.argmax(\n",
    "        model.predict([[1,0,0,0,1,1,0],\n",
    "                                [0,1,1,1,0,0,0],\n",
    "                                [1,1,0,1,1,0,1],\n",
    "                                [0,0,0,0,0,1,0],\n",
    "                                [1,0,1,0,1,1,1]]), axis=1 )))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
